"""
DATS 5750 Project 2: Intelligent Cloud Translation Assistant
Document Processor Cloud Function

Purpose:
    This Cloud Function handles the first stage of the document processing pipeline.
    It is triggered by Cloud Storage events when files are uploaded to the input bucket.
    The function intelligently routes documents based on file type:
    - Text files (.txt, .html, .md): Direct text extraction for optimal performance
    - Visual documents (.pdf, .jpg, .png): Document AI OCR processing
    
    After text extraction, the function publishes a message to Pub/Sub to trigger
    the translation stage, creating a decoupled event-driven architecture.

Key Features:
    - Multi-modal document processing (text files and visual documents)
    - Intelligent routing to optimize processing speed and cost
    - Comprehensive error handling and retry logic
    - BigQuery analytics integration for monitoring
    - Production-ready logging and observability

Author: Dennis Castro
Course: DATS 5750 - Introduction to Data Science and Analytics  
Institution: University of Pennsylvania
Generated by: Claude AI (Anthropic)
Date: February 2026
"""

import os
import json
import logging
import uuid
from datetime import datetime
from google.cloud import storage
from google.cloud import documentai_v1
from google.cloud import pubsub_v1
from google.cloud import bigquery
import functions_framework

# Configuration
PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', 'dats5750-translation-assistant')
DOCUMENT_AI_PROCESSOR_ID = "31d2c9fa6743ba82"
DOCUMENT_AI_LOCATION = "us"
PUBSUB_TOPIC = "translation-requested"

# Initialize clients
storage_client = storage.Client()
documentai_client = documentai_v1.DocumentProcessorServiceClient()
publisher = pubsub_v1.PublisherClient()
bigquery_client = bigquery.Client()

# Supported file types
TEXT_EXTENSIONS = {'.txt', '.html', '.md'}
OCR_EXTENSIONS = {'.pdf', '.jpg', '.jpeg', '.png', '.gif'}
ALL_SUPPORTED = TEXT_EXTENSIONS.union(OCR_EXTENSIONS)

def log_processing_to_bigquery(document_id, file_type, processing_method, processing_time, file_size, status):
    """Log processing metrics to BigQuery"""
    try:
        table_id = f"{PROJECT_ID}.translation_analytics.translation_logs"
        rows_to_insert = [{
            "document_id": document_id,
            "file_type": file_type,
            "processing_method": processing_method,
            "timestamp": datetime.now().isoformat(),
            "processing_time": processing_time,
            "file_size": file_size,
            "status": status
        }]
        
        table = bigquery_client.get_table(table_id)
        errors = bigquery_client.insert_rows_json(table, rows_to_insert)
        
        if not errors:
            logging.info(f"Processing metrics logged to BigQuery for {document_id}")
        else:
            logging.error(f"BigQuery insert errors: {errors}")
            
    except Exception as e:
        logging.error(f"Error logging to BigQuery: {str(e)}")

def log_error_to_bigquery(document_id, error_type, error_message):
    """Log errors to BigQuery"""
    try:
        table_id = f"{PROJECT_ID}.translation_analytics.error_logs"
        rows_to_insert = [{
            "error_id": str(uuid.uuid4()),
            "document_id": document_id,
            "error_type": error_type,
            "error_message": error_message[:1000],  # Limit message length
            "timestamp": datetime.now().isoformat()
        }]
        
        table = bigquery_client.get_table(table_id)
        errors = bigquery_client.insert_rows_json(table, rows_to_insert)
        
        if errors:
            logging.error(f"BigQuery error insert failed: {errors}")
            
    except Exception as e:
        logging.error(f"Error logging error to BigQuery: {str(e)}")

def extract_text_directly(file_content):
    """Extract text directly from text-based files"""
    try:
        # Try UTF-8 first, then fall back to other encodings
        try:
            return file_content.decode('utf-8')
        except UnicodeDecodeError:
            # Try common encodings
            for encoding in ['latin-1', 'cp1252', 'iso-8859-1']:
                try:
                    return file_content.decode(encoding)
                except UnicodeDecodeError:
                    continue
            # If all fail, use utf-8 with error replacement
            return file_content.decode('utf-8', errors='replace')
    except Exception as e:
        logging.error(f"Error in direct text extraction: {str(e)}")
        raise

def process_with_document_ai(file_content, mime_type):
    """Process documents using Document AI OCR"""
    try:
        processor_name = f"projects/{PROJECT_ID}/locations/{DOCUMENT_AI_LOCATION}/processors/{DOCUMENT_AI_PROCESSOR_ID}"
        
        # Create the request
        request = documentai_v1.ProcessRequest(
            name=processor_name,
            raw_document=documentai_v1.RawDocument(
                content=file_content,
                mime_type=mime_type
            )
        )
        
        # Process the document
        result = documentai_client.process_document(request=request)
        
        if not result.document.text.strip():
            raise ValueError("No text extracted from document")
            
        return result.document.text
    
    except Exception as e:
        logging.error(f"Document AI processing error: {str(e)}")
        raise

def store_processed_text(document_id, extracted_text, processing_method):
    """Store extracted text in processed bucket"""
    try:
        processed_bucket = storage_client.bucket(f"{PROJECT_ID}-documents-processed")
        processed_filename = f"{document_id}_processed.txt"
        
        # Create metadata content
        metadata_content = f"""Processing Method: {processing_method}
Extraction Time: {datetime.now().isoformat()}
Character Count: {len(extracted_text)}

--- EXTRACTED TEXT ---
{extracted_text}"""
        
        blob = processed_bucket.blob(processed_filename)
        blob.upload_from_string(metadata_content, content_type='text/plain; charset=utf-8')
        
        logging.info(f"Processed text stored: {processed_filename}")
        return processed_filename
        
    except Exception as e:
        logging.error(f"Error storing processed text: {str(e)}")
        raise

def publish_translation_request(document_id, extracted_text):
    """Publish message to translation topic"""
    try:
        topic_path = publisher.topic_path(PROJECT_ID, PUBSUB_TOPIC)
        
        message_data = {
            'document_id': document_id,
            'extracted_text': extracted_text[:50000],  # Limit text size for message
            'timestamp': datetime.now().isoformat()
        }
        
        # Convert to JSON and encode
        message_json = json.dumps(message_data).encode('utf-8')
        
        # Publish message
        future = publisher.publish(topic_path, message_json)
        message_id = future.result()
        
        logging.info(f"Translation request published: {message_id} for {document_id}")
        return message_id
        
    except Exception as e:
        logging.error(f"Error publishing translation request: {str(e)}")
        raise

@functions_framework.cloud_event
def process_document_upload(cloud_event):
    """Main function triggered by Cloud Storage uploads"""
    start_time = datetime.now()
    document_id = None
    
    try:
        # Extract file information from the event
        bucket_name = cloud_event.data["bucket"]
        file_name = cloud_event.data["name"]
        
        # Generate document ID from filename
        document_id = file_name.split('.')[0]  # Use filename without extension as ID
        
        logging.info(f"Processing document upload: {file_name}")
        
        # Validate file extension
        file_extension = os.path.splitext(file_name)[1].lower()
        if file_extension not in ALL_SUPPORTED:
            error_msg = f"Unsupported file type: {file_extension}"
            logging.error(error_msg)
            log_error_to_bigquery(document_id, "unsupported_file_type", error_msg)
            return
        
        # Download file from storage
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(file_name)
        
        if not blob.exists():
            error_msg = f"File not found: {file_name}"
            logging.error(error_msg)
            log_error_to_bigquery(document_id, "file_not_found", error_msg)
            return
            
        file_content = blob.download_as_bytes()
        file_size = len(file_content)
        
        # Determine MIME type
        mime_type_map = {
            '.txt': 'text/plain',
            '.html': 'text/html',
            '.md': 'text/markdown',
            '.pdf': 'application/pdf',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.gif': 'image/gif'
        }
        
        mime_type = mime_type_map.get(file_extension, 'application/octet-stream')
        
        # Smart processing based on file type
        if file_extension in TEXT_EXTENSIONS:
            # Direct text extraction - faster and more accurate
            logging.info(f"Processing {file_name} with direct text extraction")
            extracted_text = extract_text_directly(file_content)
            processing_method = "direct_text"
            
        elif file_extension in OCR_EXTENSIONS:
            # Document AI OCR processing
            logging.info(f"Processing {file_name} with Document AI OCR")
            extracted_text = process_with_document_ai(file_content, mime_type)
            processing_method = "document_ai_ocr"
            
        else:
            # This shouldn't happen due to earlier validation, but safety check
            raise ValueError(f"Unsupported file extension: {file_extension}")
        
        # Validate extracted text
        if not extracted_text or len(extracted_text.strip()) < 10:
            error_msg = f"Insufficient text extracted from {file_name}"
            logging.warning(error_msg)
            log_error_to_bigquery(document_id, "insufficient_text", error_msg)
            return
        
        # Store processed text
        store_processed_text(document_id, extracted_text, processing_method)
        
        # Publish translation request
        publish_translation_request(document_id, extracted_text)
        
        # Calculate processing time
        processing_time = int((datetime.now() - start_time).total_seconds())
        
        # Log success to BigQuery
        log_processing_to_bigquery(
            document_id, file_extension, processing_method, 
            processing_time, file_size, "processed"
        )
        
        logging.info(f"Successfully processed {file_name} in {processing_time} seconds")
        
    except Exception as e:
        error_msg = f"Processing failed for {document_id}: {str(e)}"
        logging.error(error_msg)
        
        if document_id:
            log_error_to_bigquery(document_id, "processing_error", str(e))
        
        # Don't re-raise exception to prevent Cloud Function retries
        return

if __name__ == "__main__":
    # For local testing
    logging.basicConfig(level=logging.INFO)
    print("Document processor function loaded successfully")
