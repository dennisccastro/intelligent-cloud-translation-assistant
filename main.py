"""
DATS 5750 Project 2: Intelligent Cloud Translation Assistant
Translation Processor Cloud Function

Purpose:
    This Cloud Function handles the second stage of the document processing pipeline.
    It is triggered by Pub/Sub messages containing extracted text from the document
    processor. The function performs language detection and neural machine translation
    using Google's Translation API.
    
    The function implements robust error handling for CloudEvent message parsing,
    accommodating both 1st and 2nd generation Cloud Functions formats. After
    successful translation, results are stored with comprehensive metadata and
    analytics are logged to BigQuery.

Key Features:
    - Automatic language detection with confidence scoring
    - Neural machine translation (currently English → Spanish)
    - Robust CloudEvent message parsing with fallback handling
    - Comprehensive metadata storage with translation results
    - BigQuery analytics integration and error tracking
    - Production-ready logging and monitoring

Author: Dennis Castro
Course: DATS 5750 - Introduction to Data Science and Analytics
Institution: University of Pennsylvania  
Generated by: Claude AI (Anthropic)
Date: February 2026
"""

import os
import json
import logging
import uuid
import base64
from datetime import datetime
from google.cloud import storage
from google.cloud import translate_v2 as translate
from google.cloud import pubsub_v1
from google.cloud import bigquery
import functions_framework

# Configuration
PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', 'dats5750-translation-assistant')
COMPLETION_TOPIC = "translation-completed"

# Initialize clients
storage_client = storage.Client()
translate_client = translate.Client()
publisher = pubsub_v1.PublisherClient()
bigquery_client = bigquery.Client()

def log_translation_to_bigquery(document_id, source_language, target_language, 
                               processing_time, text_length, status):
    """Log translation metrics to BigQuery"""
    try:
        table_id = f"{PROJECT_ID}.translation_analytics.translation_logs"
        rows_to_insert = [{
            "document_id": document_id,
            "source_language": source_language,
            "target_language": target_language,
            "timestamp": datetime.now().isoformat(),
            "processing_time": processing_time,
            "text_length": text_length,
            "status": status
        }]
        
        table = bigquery_client.get_table(table_id)
        errors = bigquery_client.insert_rows_json(table, rows_to_insert)
        
        if not errors:
            logging.info(f"Translation metrics logged to BigQuery for {document_id}")
        else:
            logging.error(f"BigQuery insert errors: {errors}")
            
    except Exception as e:
        logging.error(f"Error logging translation to BigQuery: {str(e)}")

def log_error_to_bigquery(document_id, error_type, error_message):
    """Log errors to BigQuery"""
    try:
        table_id = f"{PROJECT_ID}.translation_analytics.error_logs"
        rows_to_insert = [{
            "error_id": str(uuid.uuid4()),
            "document_id": document_id,
            "error_type": error_type,
            "error_message": error_message[:1000],  # Limit message length
            "timestamp": datetime.now().isoformat()
        }]
        
        table = bigquery_client.get_table(table_id)
        errors = bigquery_client.insert_rows_json(table, rows_to_insert)
        
        if errors:
            logging.error(f"BigQuery error insert failed: {errors}")
            
    except Exception as e:
        logging.error(f"Error logging error to BigQuery: {str(e)}")

def get_full_text_from_storage(document_id):
    """Retrieve full processed text from storage if needed"""
    try:
        processed_bucket = storage_client.bucket(f"{PROJECT_ID}-documents-processed")
        processed_filename = f"{document_id}_processed.txt"
        
        blob = processed_bucket.blob(processed_filename)
        if not blob.exists():
            return None
            
        content = blob.download_as_text(encoding='utf-8')
        
        # Extract just the text content (after the metadata)
        if "--- EXTRACTED TEXT ---" in content:
            return content.split("--- EXTRACTED TEXT ---", 1)[1].strip()
        else:
            return content.strip()
            
    except Exception as e:
        logging.error(f"Error retrieving full text from storage: {str(e)}")
        return None

def detect_language_with_fallback(text):
    """Detect language with fallback handling"""
    try:
        # Use first 1000 characters for detection to save API costs
        sample_text = text[:1000] if len(text) > 1000 else text
        
        detection = translate_client.detect_language(sample_text)
        
        detected_language = detection['language']
        confidence = detection.get('confidence', 0.0)
        
        logging.info(f"Detected language: {detected_language} (confidence: {confidence})")
        
        # If confidence is very low, default to English
        if confidence < 0.5:
            logging.warning(f"Low confidence language detection ({confidence}), defaulting to English")
            detected_language = 'en'
            
        return detected_language, confidence
        
    except Exception as e:
        logging.error(f"Language detection error: {str(e)}, defaulting to English")
        return 'en', 0.0

def translate_text_content(text, target_language='es', source_language=None):
    """Translate text content with error handling"""
    try:
        if not source_language:
            source_language, confidence = detect_language_with_fallback(text)
        
        # Skip translation if source and target are the same
        if source_language == target_language:
            logging.info(f"Source and target languages are the same ({source_language}), skipping translation")
            return {
                'translatedText': text,
                'detectedSourceLanguage': source_language,
                'confidence': 1.0
            }
        
        # Perform translation
        result = translate_client.translate(
            text,
            target_language=target_language,
            source_language=source_language
        )
        
        return {
            'translatedText': result['translatedText'],
            'detectedSourceLanguage': result.get('detectedSourceLanguage', source_language),
            'confidence': confidence if 'confidence' in locals() else None
        }
        
    except Exception as e:
        logging.error(f"Translation error: {str(e)}")
        raise

def store_translation_result(document_id, translation_result, source_language, target_language):
    """Store translation result in output bucket"""
    try:
        output_bucket = storage_client.bucket(f"{PROJECT_ID}-translations-output")
        translated_filename = f"{document_id}_{target_language}.txt"
        
        # Create comprehensive metadata
        metadata_content = f"""Translation Report
==================
Document ID: {document_id}
Source Language: {source_language}
Target Language: {target_language}
Translation Confidence: {translation_result.get('confidence', 'N/A')}
Processed: {datetime.now().isoformat()}
Character Count: {len(translation_result['translatedText'])}

--- TRANSLATED CONTENT ---
{translation_result['translatedText']}"""
        
        # Upload to storage
        blob = output_bucket.blob(translated_filename)
        blob.upload_from_string(
            metadata_content, 
            content_type='text/plain; charset=utf-8'
        )
        
        # Set metadata
        blob.metadata = {
            'document_id': document_id,
            'source_language': source_language,
            'target_language': target_language,
            'processed_time': datetime.now().isoformat()
        }
        blob.patch()
        
        logging.info(f"Translation result stored: {translated_filename}")
        return translated_filename
        
    except Exception as e:
        logging.error(f"Error storing translation result: {str(e)}")
        raise

def publish_completion_notification(document_id, status, filename=None):
    """Publish completion notification"""
    try:
        topic_path = publisher.topic_path(PROJECT_ID, COMPLETION_TOPIC)
        
        notification_data = {
            'document_id': document_id,
            'status': status,
            'filename': filename,
            'timestamp': datetime.now().isoformat()
        }
        
        # Convert to JSON and encode
        message_json = json.dumps(notification_data).encode('utf-8')
        
        # Publish message
        future = publisher.publish(topic_path, message_json)
        message_id = future.result()
        
        logging.info(f"Completion notification published: {message_id} for {document_id}")
        return message_id
        
    except Exception as e:
        logging.error(f"Error publishing completion notification: {str(e)}")
        # Don't raise exception as this is not critical

@functions_framework.cloud_event
def translate_document(cloud_event):
    """Main function triggered by Pub/Sub messages"""
    start_time = datetime.now()
    document_id = None
    
    try:
        logging.info(f"Received translation request: {cloud_event.data}")
        
        # Parse the Pub/Sub message - handle different CloudEvent formats
        try:
            # Try direct access first (2nd gen functions)
            message_data = json.loads(cloud_event.data['message']['data'])
        except (TypeError, AttributeError, KeyError):
            # Fallback for base64 encoded data (1st gen functions)
            try:
                message_data = json.loads(base64.b64decode(cloud_event.data['message']['data']).decode('utf-8'))
            except:
                # Last resort: try direct data access
                message_data = json.loads(cloud_event.data)
        
        logging.info(f"Parsed message data keys: {list(message_data.keys())}")
        
        # Extract document information
        document_id = message_data.get('document_id')
        extracted_text = message_data.get('extracted_text', '')
        
        if not document_id:
            error_msg = "No document_id found in message"
            logging.error(error_msg)
            return
            
        if not extracted_text or len(extracted_text.strip()) == 0:
            logging.warning(f"No text content in message for {document_id}, attempting to retrieve from storage")
            extracted_text = get_full_text_from_storage(document_id)
            
            if not extracted_text or len(extracted_text.strip()) < 10:
                error_msg = f"No sufficient text content found for {document_id}"
                logging.error(error_msg)
                log_error_to_bigquery(document_id, "no_text_content", error_msg)
                return
        
        logging.info(f"Processing translation for {document_id} ({len(extracted_text)} characters)")
        
        # Perform language detection and translation
        target_language = 'es'  # Spanish - can be made configurable
        translation_result = translate_text_content(extracted_text, target_language)
        
        detected_language = translation_result['detectedSourceLanguage']
        translated_text = translation_result['translatedText']
        
        # Validate translation result
        if not translated_text or len(translated_text.strip()) == 0:
            error_msg = f"Translation produced empty result for {document_id}"
            logging.error(error_msg)
            log_error_to_bigquery(document_id, "empty_translation", error_msg)
            return
        
        # Store translation result
        output_filename = store_translation_result(
            document_id, translation_result, detected_language, target_language
        )
        
        # Calculate processing time
        processing_time = int((datetime.now() - start_time).total_seconds())
        
        # Log success to BigQuery
        log_translation_to_bigquery(
            document_id, detected_language, target_language,
            processing_time, len(extracted_text), "completed"
        )
        
        # Publish completion notification
        publish_completion_notification(document_id, "completed", output_filename)
        
        logging.info(f"Successfully translated {document_id} ({detected_language} → {target_language}) in {processing_time} seconds")
        
    except Exception as e:
        error_msg = f"Translation failed for {document_id}: {str(e)}"
        logging.error(error_msg)
        
        if document_id:
            log_error_to_bigquery(document_id, "translation_error", str(e))
            publish_completion_notification(document_id, "failed")
        
        # Don't re-raise exception to prevent Cloud Function retries
        return

if __name__ == "__main__":
    # For local testing
    logging.basicConfig(level=logging.INFO)
    print("Translation processor function loaded successfully")
